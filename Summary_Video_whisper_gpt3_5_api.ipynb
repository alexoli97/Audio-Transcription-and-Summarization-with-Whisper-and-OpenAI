{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Audio Transcription and Summarization with Whisper and OpenAI\n",
        "\n",
        "## Description\n",
        "This program is a tool for automatically transcribing and summarizing the content of audio and video files. It uses Whisper's voice recognition technology for transcription and OpenAI's API for generating summaries. It is designed to be run in a Google Colab environment.\n",
        "\n",
        "## Features\n",
        "- **Audio/Video Transcription**: Uses the Whisper model to transcribe audio extracted from video files.\n",
        "- **Automatic Language Detection**: Ability to detect the language of the audio for accurate transcription.\n",
        "- **Summary Generation**: Uses the OpenAI API to summarize the transcribed text, ideal for meetings, conferences, etc.\n",
        "\n",
        "## Use of the OpenAI API\n",
        "This program makes use of the OpenAI API, which is a paid service. Please **use this API responsibly** to avoid excessive charges.\n",
        "\n",
        "## GPU Configuration\n",
        "It's crucial to ensure that you are using a GPU in Google Colab for optimal performance. To activate the GPU:\n",
        "   - Go to `Runtime` > `Change runtime type`.\n",
        "   - Select `T4 GPU` in the hardware accelerator option.\n",
        "   - Save and proceed to run the notebook.\n",
        "\n",
        "## Usage Instructions\n",
        "1. **Video Upload**: Upload your video file to the Colab environment.\n",
        "2. **Video Configuration**: Update the path in the script to match the location of your uploaded video.\n",
        "3. **API Configuration**: Verify that the OpenAI API key is correctly configured in the environment.\n",
        "4. **Script Execution**: Go to `Runtime` > `Run all`.\n",
        "5. **Access to Results**: The transcribed and summarized texts will automatically be saved in text files within the Colab environment.\n",
        "\n",
        "## Dependencies\n",
        "- **Whisper**: Advanced voice recognition.\n",
        "- **moviepy**: Video editing and processing.\n",
        "- **ffmpeg**: Conversion and manipulation of multimedia files.\n",
        "- **tiktoken**: Authentication token management.\n",
        "- **openai**: Interface for OpenAI language models (GPT-3-5 in this case).\n",
        "\n",
        "## Authors and Acknowledgments\n",
        "This script has been developed by Alejandro Oliveros Ord√°s with the help of OpenAI and Whisper technologies.\n"
      ],
      "metadata": {
        "id": "FWUAR0jmqfUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = #your api key\n",
        "\n",
        "video_path = #your video path"
      ],
      "metadata": {
        "id": "bsyWqI_9ivoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Check if CUDA is available for GPU use\n",
        "torch.cuda.is_available()\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "re8_1glpwk-N",
        "outputId": "923b50ac-f446-4784-a1a3-e014d11de4f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fi9E4zHi1foa",
        "outputId": "d62a6530-987f-4f62-9674-0d6546000e62",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-o46h1hkh\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-o46h1hkh\n",
            "  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=802825 sha256=e878f0edec8c072ef02dc416f9af4ea7c31314510f1a493d8297126a8841d115\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8mwt3s4y/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: openai-whisper\n",
            "  Attempting uninstall: openai-whisper\n",
            "    Found existing installation: openai-whisper 20231117\n",
            "    Uninstalling openai-whisper-20231117:\n",
            "      Successfully uninstalled openai-whisper-20231117\n",
            "Successfully installed openai-whisper-20231117\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.31.0)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.1.10)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from moviepy) (1.23.5)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.31.6)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.4.9)\n",
            "Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy) (9.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg>=0.2.0->moviepy) (67.7.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2023.11.17)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n",
            "Collecting tiktoken\n",
            "  Using cached tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.11.17)\n",
            "Installing collected packages: tiktoken\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tiktoken-0.5.2\n",
            "Collecting openai\n",
            "  Using cached openai-1.5.0-py3-none-any.whl (223 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Using cached httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Using cached httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 openai-1.5.0\n"
          ]
        }
      ],
      "source": [
        "# @title Installing Dependencies\n",
        "!pip install --upgrade --no-deps --force-reinstall git+https://github.com/openai/whisper.git\n",
        "!pip install moviepy\n",
        "!apt install ffmpeg\n",
        "!pip install tiktoken\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5qZjzEA1fod",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Import Dependencies\n",
        "import whisper\n",
        "import numpy as np\n",
        "import subprocess\n",
        "import io\n",
        "import requests\n",
        "from openai import OpenAI\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TL7Omun11foe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54d5bb74-84d8-4233-ff43-fe4ac98e5702",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.42G/1.42G [00:15<00:00, 100MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model is multilingual and has 762,321,920 parameters.\n"
          ]
        }
      ],
      "source": [
        "# @title Load the Whisper model\n",
        "\n",
        "# Cargar el modelo de Whisper\n",
        "model = whisper.load_model(\"medium\", device=DEVICE)\n",
        "print(\n",
        "    f\"Model is {'multilingual' if model.is_multilingual else 'English-only'} \"\n",
        "    f\"and has {sum(np.prod(p.shape) for p in model.parameters()):,} parameters.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Upload video and extract audio\n",
        "# Cargar el video y extraer el audio usando ffmpeg\n",
        "extracted_audio_path = '/content/extracted_audio.mp3'  # Ruta para el archivo de audio extra√≠do\n",
        "command = ['ffmpeg', '-i', video_path, '-f', 'mp3', extracted_audio_path]\n",
        "subprocess.run(command)"
      ],
      "metadata": {
        "id": "r_zI7fuhUKcG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d4069ec-8c59-4857-ce82-3041b46d2427",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CompletedProcess(args=['ffmpeg', '-i', '/content/drive/MyDrive/MyBS/wf_rotado_no_va.mkv', '-f', 'mp3', '/content/extracted_audio.mp3'], returncode=0)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Process audio and detect language\n",
        "\n",
        "# Procesar el audio\n",
        "audio = whisper.load_audio(extracted_audio_path)\n",
        "audio = whisper.pad_or_trim(audio)\n",
        "mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "\n",
        "# Detectar el idioma del audio\n",
        "_, probs = model.detect_language(mel)\n",
        "detected_language = max(probs, key=probs.get)\n",
        "print(f\"Detected language: {detected_language}\")"
      ],
      "metadata": {
        "id": "43WwzFLFWj2F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "605d2a2b-bbaa-4ade-ebfd-9a65a0d48b07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected language: es\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDKhk1xR1fof",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "b7aae7f9-83be-4d86-9cf4-9f712f73b5c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " a ver lo que he visto esta no es la que dec√≠a yo la que acaban que en ese dice el proceso el proceso derrotado de folder a pe folder como que no arranca vale este es el proceso que pasa los desarrolladores dejan sus cosas en carpeta e y autom√°ticamente se pasa carpeta p si ha ido bien vale entonces si no salta suele ser porque el proceso que se encarga de ello que est√° aqu√≠ en p y m utilidades vale este rotado carpetas por lo que sea mejor se ha quedado pillado en teor√≠a yo lo que he hecho ha sido darle a un restar al proceso que ya ha acabado al cliente le puede decir que ya ha acabado pero hay que ver es que pilla la planificaci√≥n porque si ponen otro vamos a estar en la misma vale entonces el rollo es aqu√≠ no es que duro pues te vienes aqu√≠ haces un versi√≥n check out para poder manipular el workflow vale te vas a workflow ed y schedule es schedule aqu√≠ schedule y aqu√≠ viene la digamos cuando como ejecutarse entonces despu√©s de ponerle que empiece yo no s√© por qu√© supone que esto normalmente debe estar siempre corriendo y si le das aqu√≠ que corra cada minuto deber√≠a bastar sabes para que ya se se pongan m√°s vale sabe que para sabes que estaba puesto que no se ejecutaba o no estaba puesto bien pero es que no s√© hay veces que se pierde la planificaci√≥n ahora parece que s√≠ que va a correr bueno habr√≠a que esto te vas aqu√≠ a la 9.02 si tiene su propia carpeta de utilidades rotarte folder se llama rotarte folder aqu√≠ vale que vas al lado no son rotados d√≠a el √∫ltimo esto es que ha corrido yo creo que si pongo resumen rotado de otro d√≠a pone todas as√≠ ah√≠ pone todas vamos a ver ahora si mete una entrada sabes si hay 55 que no me ha puesto no me parece o hay 50 pues habr√≠a que ver aqu√≠ que escribe y que luego va a escribir 56 y as√≠ entonces ya vemos que vamos si quieres esto dejalo al usuario y le pues ya tu rotado ya tal vez hemos revisado y ya me dejo esto no dec√≠a y esto voy a echar lo dejo yo aqu√≠ abierto y reviso que ya va vale lo que te iba a decir es que las 9.24 del d√≠a de hoy se ve que si son rotados s√≠ pero por eso he llegado yo aqu√≠ y le hecho aqu√≠ est√° entonces se ha ejecutado fuera de planificaci√≥n en directamente lo he ejecutado para que fuera lo del esto te voy a pasar aqu√≠ el lo y sobre la otra petici√≥n de ayer comentado con tom√°s porque √©l ha estado m√°s con el tema de jaz y que de hecho un ojo si no luego lo vemos los tres vale una cosa importante no o sea no te dejes no te dejes peticiones aqu√≠ corriendo porque quiere decir que a ver que no es culpa tuyo mucho que por ejemplo est√° estas que tienes aqu√≠ esta no es de que es y es pero esas son de cosas que que hab√≠a que mirar todav√≠a de permisos de usuarios que justo me han dado ya vale pero a lo mejor aqu√≠ las puedes poner no me acuerdo c√≥mo era creo que las puedes poner en pause o algo as√≠ para que no te corra esto aqu√≠ abajo que tiene un rollo de un no parece aqu√≠ no parece pero que luego pueden decirnos oye que ha pasado el tiempo de de respuesta entonces est√° igual o sea tenemos un tiempo para atenderla entonces a ver si este maestro tampoco es plan o sea que no digas la si no puede decir a t√∫ como tu labor tambi√©n aparte del buz√≥n despu√©s de decir luego en nuestro chat interno y tenemos que revisar esta y nos mencionas a todos o lo que sea sabes y pero que no te las vayas dejando aqu√≠ sabes muerta porque luego a lo mejor como va corriendo el tiempo no os pueden decir algo o lo que sea sabes y est√° la puedes cerrar le dice lo que hemos hablado y luego eso pues no comentan m√°s rollo interno nuestro si este maestro que no es de algo de Antonio pues yo que s√© puede decir bueno pues luego vemos esta petici√≥n que lleg√≥ y para que no sea t√∫ una de tus labores y hago si tambi√©n se lo puedes transmitir pues es decir un poco de cien de co√±o aqu√≠ no se me queda nada cola o hay esto que tenemos pendiente lo que sea vale vale por nada si quieres cierre a esta y te he pasado aqu√≠ a ver si te lo he pasado para que veas lo derrotado donde es si quieres intenta tambi√©n t√∫ luego cuando eso acepto un check out ver todo esto que hemos visto que te acabo ense√±ar y si tienes dudas lo volvemos a ver porque hay varios de estos que est√°n todos programados si los ves por aqu√≠ hacen distintas tareas vez que ponen el ch√©vol si est√° funcionando mejor no tocarlo si si no si es s√≥lo para que veas t√∫ que mira pero yo creo que ya mira ya est√° corriendo cada minuto ves ya si vemos el lo ves ya va escribiendo cada minuto o sea que ya no quiere pasar a ser solucionado esto es para que pues no nos hagan tantas peticiones ya con este autom√°ticamente pues van a hacer vale vale pues nada\n",
            "Result of transcription text saved to: /content/transcription.txt\n"
          ]
        }
      ],
      "source": [
        "# @title Transcribe with Whisper and save output to transcription.txt\n",
        "\n",
        "# Opciones para la transcripci√≥n utilizando el idioma detectado\n",
        "options = whisper.DecodingOptions(language=detected_language, without_timestamps=True, fp16=False)\n",
        "result = whisper.decode(model, mel, options)\n",
        "\n",
        "result = model.transcribe(extracted_audio_path)\n",
        "print(result[\"text\"])\n",
        "\n",
        "# Guardar la transcripci√≥n en .txt\n",
        "result_text = result[\"text\"]\n",
        "file_path = '/content/transcription.txt'\n",
        "\n",
        "with open(file_path, 'w') as file:\n",
        "    file.write(result_text)\n",
        "\n",
        "print(f'Result of transcription text saved to: {file_path}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Summarise the transcript with the GPT-3.5 API and save the output to summary_transcription.txt\n",
        "\n",
        "def summarize_text(text):\n",
        "    client = OpenAI(\n",
        "        api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
        "    )\n",
        "\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"Tu tarea es generar un resumen completo y claro de reuniones de trabajo transcritas. El resumen debe incluir los puntos clave discutidos, las decisiones tomadas y cualquier acci√≥n o tarea asignada. Evita detalles innecesarios y enf√≥cate en la esencia de la reuni√≥n. El resumen debe ser lo suficientemente informativo para alguien que no asisti√≥ a la reuni√≥n.\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Por favor, resume la siguiente transcripci√≥n de la reuni√≥n: \" + text\n",
        "    }\n",
        "]\n",
        "    )\n",
        "\n",
        "    # Accede a la respuesta de la siguiente manera\n",
        "    resumen = chat_completion.choices[0].message.content.strip()\n",
        "    return resumen\n",
        "\n",
        "# Uso de la funci√≥n de resumen\n",
        "result_text = result[\"text\"]\n",
        "summary = summarize_text(result_text)\n",
        "\n",
        "# El resumen hecho por gpt-3.5\n",
        "print(summary)\n",
        "\n",
        "# Guardar el resumen en un archivo .txt\n",
        "file_path = '/content/summary_transcription.txt'\n",
        "with open(file_path, 'w') as file:\n",
        "    file.write(summary)\n",
        "\n",
        "print(f'Summary text saved to: {file_path}')"
      ],
      "metadata": {
        "id": "kqc0yWXXRbNd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "46f15b5d-e29b-41eb-bf2c-c4e36eb78857"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La reuni√≥n tuvo lugar para discutir el proceso de rotaci√≥n de carpetas en el trabajo de los desarrolladores. Se mencion√≥ que a veces el proceso se queda pillado en la carpeta de utilidades y no se ejecuta correctamente. Se decidi√≥ reiniciar el proceso e informar al cliente una vez que haya finalizado. Tambi√©n se discuti√≥ el manejo de las peticiones pendientes y se acord√≥ no dejarlas corriendo indefinidamente, sino ponerlas en pausa si es necesario. Se asign√≥ la tarea de revisar las peticiones con el equipo y mencionar cualquier problema en nuestro chat interno. Adem√°s, se mencion√≥ una solicitud relacionada con el tema de Jaz que se abordar√≠a m√°s adelante. Se comparti√≥ una carpeta con archivos sobre el proceso de rotaci√≥n para que todos los participantes pudieran revisarlos y resolver cualquier duda.\n",
            "Summary text saved to: /content/summary_transcription.txt\n"
          ]
        }
      ]
    }
  ]
}